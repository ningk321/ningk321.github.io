---
title: "\U0001F44B Exploring hand gestural control for mobile devices"
layout: post
date: 2019-08-11 22:00
tag:
- Cross-device
- Data Visualisation
- Statistical test
- Exit Interview
- Questionnaires
- Mixed-method
projects: true
category: project
author: chloeng
description: Gesture Elicitation
---

## Eliciting User-defined Touch and Mid-air Gestures for Co-located Mobile Gaming
<b>
<b>Nature: </b> Academic research

<b>
<b>Type: </b> Individual project

<b>
<b>Methodologies: </b> Questionnaires, interviews, think-aloud, survey, thematic analysis, data visualisation, descriptive statistics, t-tests

<b>
<b>Duration: </b> 4 months

<b>
<b>My role: </b> I managed the entire project starting from literature review of past gesture elicitation studies and gestural control. Following the study design, which includes survey design, artifact design, interview script, I went on recruiting participants and carrying out the lab sessions. After collecting video recordings of the elicitation and survey response, I coded the quant/qual data for analysis, subsequently writing up the report.


<br>
<br>
<br>
<br>
<br>
![example gestures](https://chloenhy.github.io/assets/images/gesture/example-gesture.jpg)
<figcaption>Examples of mid-air gestures from the study</figcaption>
<br>
<br>
### Background
<b>
In recent years, mobile games have become increasingly popular and have largely improved on their interaction techniques. This improvement is enabled by the increasing capability in modern mobile devices as they feature sophisticated sensors such as accelerometers, gyroscope, and motion sensors, which allows for a vast range of input methods. 

<b>
To make use of the improving capability in mobile devices in the domain of mobile games, there has been research that explores alternative input methods (other than using capacitive touchscreens).
<br>
<br>
<br>
<br>
### Research Approach
<b>
In this research, I aim <span class="evidence">to explore the use of gesture controls in co-located mobile gaming, an area that has not been focused on in the industry and research community</span>. I will explore traditional multiplayer tabletop games such as board and card games due to their clearly defined game tasks, and the communicative nature of the game, and the materiality of the game materials cherished by players in a co-located setting.
<br>
<br>
<br>
<br>
### Methodology
<b>
I draw upon the widely adopted gesture elicitation methodology to understand user mental models and help develop user-defined gestures. 

<b>User elicitation is one form of participatory design to include users’ mental models and proposals in designing new interaction techniques. <span class="evidence">Elicitation studies aim to invoke easy-to-learn and memorable user-defined gestures</span> instead of gestures that are optimised for machine recognition. In an elicitation, participants are asked to propose gestures to achieve tasks (known as referents) in a specified modality.

<br>
<div style="display: flex; justify-content: center;">
    <img alt="study setup" src="https://chloenhy.github.io/assets/images/gesture/study-setup.jpg" width="50%" height="50%" />
</div>
<br>

<b>
I conducted a gesture elicitation study for tasks common in a multiplayer card and board game moderated by mobile devices. I recruited 24 participants in pairs. Twelve were working professionals different backgrounds such as analytics, marketing, engineering, clinical settings, and sports. The other twelve were university students in various disciplines.
<br>
<br>
<div style="display: flex; justify-content: center;">
<img alt="participants" src="https://chloenhy.github.io/assets/images/gesture/participants.jpg" width="60%" height="60%" align="middle"/>
</div>
<br>
<figcaption>One of the participant pairs proposing gestures</figcaption>
<br>
<br>
<b>Research question: 

<b><b>How can the results and observations made in a gesture elicitation for game tasks inform gesture design for co-located multiplayer mobile games?</b>
<br>
<br>
<br>
<br>
### Data Collection and Analysis
<b>Data collected include <b>observational notes</b> jotted by the experimenter and the <b>video recordings</b> of the entire sessions, including the elicitation and the post-study interviews. The <b>pre-</b> and <b>post-study questionnaires</b> contain demographic, technology usage habits, Likert-scale and free-form responses. <b>Descriptive statistics</b> were generated from the Likert-scale questions. 

<b>All gestures were coded and fed into the <a href="http://depts.washington.edu/acelab/proj/dollar/agate.html" target="_blank">AGAte 2.0 tool</a> for <b>agreement rate</b> calculation.
<br>
<br>
![AGAte 2.0 tool screenshot](https://chloenhy.github.io/assets/images/gesture/agate.jpg)
<figcaption>Screenshot of the AGAte software for analysing the agreement rates among gestures</figcaption>
<br>
<br>
<br>
<br>
### Results
<b>The final consensus gesture set:
<div style="display: flex; justify-content: center;">
<img alt="mid air gestures" src="https://chloenhy.github.io/assets/images/gesture/mid-air-gesture.jpg"/>
</div>
<div style="display: flex; justify-content: center;">
<img alt="touch gestures" src="https://chloenhy.github.io/assets/images/gesture/touch-gesture.jpg"/>
</div>




#### Quantitative findings
<b>A total of 662 gesture proposals, with 286 distinct gestures (by referents) were collected. t-tests were performed to compare gesture proposals between Mid-air and Touch modality.


<b><b>Agreement rates</b>
- <b>As expected, the average agreement for Touch gestures (ARtouch=0.215) was higher than that for Mid-air gestures (ARmid-air=0.101) i.e.  a wider variety of gesture proposals in the Mid-air modality than in the Touch modality.

- <b>Two dichotomous pairs – Give cardtouch / Take cardtouch and Give chiptouch / Take chiptouch show a significant difference in agreement rates. However, we did not see a specific trend in these differences.

<br>
<br>

![bar chart of the agreement rates](https://chloenhy.github.io/assets/images/gesture/agreement-rates.jpg)


<br>
<br>
<br>
<b><b>Subjective rating</b>
- <b>In general, participants found it fun to propose gesture by themselves (mean=6.25, SD=0.77) and with a partner (mean=6.55, SD=0.81). 

- <b>However, 7 participants found it difficult to suggest gestures (mean=3.41, SD=1.73).

- <b>Regarding the final gestures chosen favourite, participants agreed that the set was a good fit for its purpose (mean=6.46, SD=0.58), and to a lesser extent, learnable (mean=5.67, SD=1.40) and easy to perform (mean=5.92, SD=0.95).

- <b> Response was mixed for whether the gestures were tiring to perform (mean=3.5, SD=1.80).

<div style="display: flex; justify-content: center;">
  <img alt="survey" src="https://chloenhy.github.io/assets/images/gesture/gesture-survey.jpg" width="60%" height="60%" align="middle"/>
</div>
<br>
<br>
<br>
<br>
<b><b>Taxonomy</b>

<b>Findings from the taxonomy are omitted in this post.
<div style="display: flex; justify-content: center;">
  <img alt="survey" src="https://chloenhy.github.io/assets/images/gesture/overall-taxonomy.jpg" width="60%" height="60%" align="middle"/>
</div>

<div style="display: flex; justify-content: center;">
  <img alt="survey" src="https://chloenhy.github.io/assets/images/gesture/nature-temporalflow.jpg" width="60%" height="60%" align="middle"/>
</div>

<div style="display: flex; justify-content: center;">
  <img alt="survey" src="https://chloenhy.github.io/assets/images/gesture/actor.jpg" width="60%" height="60%" align="middle"/>
</div>
<br>
<br>
<br>
<br>
#### Qualitative findings
<b> Six themes emerged from the thematic analysis of interviews, think-aloud data during the elicitation and written comments in our post-study questionnaires.


- <b>All participants welcomed the idea of using gestures to control mobile devices in a board or card game, but to various extent.

- <b>They welcomed gestural control due to the novelty of alternative gestures and the potential of gesture to streamline the game flow.

- <b>Besides the inclination for “something other than touch” in games, participants preferred gestures that give them a sense of achievement and foster interaction between players. 

<div style="display: flex; justify-content: center;">
    <img alt="collaborative gestures" src="https://chloenhy.github.io/assets/images/gesture/collab-gesture.jpg" width="50%" height="50%" />
</div>


- <b>Participants recognised realistic gestures can assist the game by forming shared situation awareness in all players.

- <b>Participants were enthusiastic about proposing artefact- manipulative gestures and frequently described them as “fun”.

- <b>Participants considered social etiquette when using gesture for a referent that involves a partner.

<br>
<div style="display: flex; justify-content: center;">
    <img alt="transfer gestures" src="https://chloenhy.github.io/assets/images/gesture/transfer-gesture.jpg" width="50%" height="50%" />
</div>

- <b>Similar to prior studies, legacy bias affect how participants propose gestures.



### Implication
<b>Here are some brief design implication from the study.

<b><b>On Input Modality</b>
- <b> The agreement rates of each referent/modality combination can give insights into the suitable modality for a referent

<b><b>On Gesture Design</b>
- <b> Consider using gestures to enhance situation awareness.

- <b> Take advantage of modalities beyond touch to enrich the co- located gaming experience for all stakeholders, as well as to be inclusive to novice players with the educational opportunities provided by gestural input.

- <b> Design a cohesive gesture set that is available across different form factors

- <b> Explore explicit visual feedback to guide users in their action, as feedback can help users build their understanding of the gesture-based system.

<b><b>On Sensing Technology</b>
- <b> Use multiple inputs concurrently or in sequence to recognise complete compound gestures.

- <b>  Be resilient to the differences in the number of fingers for touch gesture

<br>
<br>
<div class="breaker"></div>
<br>
<br>
<b>This post serves as an overview of some main points in the paper that some bits here and there are omitted. The detailed explanation of the participants, procedure, artifacts, agreement rates, taxonomy and gestures are presented in the full paper. Feel free to drop me a message if you are interested!